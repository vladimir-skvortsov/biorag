{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import transformers\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "  'cuda'\n",
    "  if torch.cuda.is_available()\n",
    "  else 'mps'\n",
    "  if torch.backends.mps.is_available()\n",
    "  else 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def preprocess(corpus):\n",
    "  corpus = corpus.lower()\n",
    "  stopset = nltk.corpus.stopwords.words('english') + nltk.corpus.stopwords.words('russian') + list(string.punctuation)\n",
    "  tokens = nltk.word_tokenize(corpus)\n",
    "  tokens = [t for t in tokens if t not in stopset]\n",
    "  tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "  corpus = ' '.join(tokens)\n",
    "  corpus = unidecode(corpus)\n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model='llama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_cosine_sim_metric(expected_answers, predicted_answers):\n",
    "  results = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    expected_embedding = np.array(embeddings.embed_query(expected_answer))\n",
    "    predicted_embedding = np.array(embeddings.embed_query(predicted_answer))\n",
    "\n",
    "    sim = cosine_similarity(\n",
    "      expected_embedding.reshape(1, -1),\n",
    "      predicted_embedding.reshape(1, -1),\n",
    "    )[0][0]\n",
    "\n",
    "    results.append(sim)\n",
    "\n",
    "  return np.mean(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Following endaerectomy on the right common car...</td>\n",
       "      <td>Central aery of the retina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hypomimia is ?</td>\n",
       "      <td>Deficit of expression by gesture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>With which of the following receptors theophyl...</td>\n",
       "      <td>Adenosine receptors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>During Sx for meningioma, the left paracentral...</td>\n",
       "      <td>Rt. Leg and perineus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All of the following structure lies outside th...</td>\n",
       "      <td>Maxillary nerve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>What is the purpose of gephyrin in the glycine...</td>\n",
       "      <td>Involved in anchoring the receptor to a specif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>What is the glycine receptor involved in ?</td>\n",
       "      <td>Reflex response\\nCauses reciprocal inhibition ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>What happens in hyperperplexia ?</td>\n",
       "      <td>It’s an exaggerated reflex Often caused by a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>What is hyperperplexia treated with ?</td>\n",
       "      <td>Benzodiazepine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>What increases glycine release ?</td>\n",
       "      <td>Tetanus toxin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2076 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0     Following endaerectomy on the right common car...   \n",
       "1                                        Hypomimia is ?   \n",
       "2     With which of the following receptors theophyl...   \n",
       "3     During Sx for meningioma, the left paracentral...   \n",
       "4     All of the following structure lies outside th...   \n",
       "...                                                 ...   \n",
       "2071  What is the purpose of gephyrin in the glycine...   \n",
       "2072         What is the glycine receptor involved in ?   \n",
       "2073                   What happens in hyperperplexia ?   \n",
       "2074              What is hyperperplexia treated with ?   \n",
       "2075                   What increases glycine release ?   \n",
       "\n",
       "                                                 answer  \n",
       "0                            Central aery of the retina  \n",
       "1                      Deficit of expression by gesture  \n",
       "2                                   Adenosine receptors  \n",
       "3                                  Rt. Leg and perineus  \n",
       "4                                       Maxillary nerve  \n",
       "...                                                 ...  \n",
       "2071  Involved in anchoring the receptor to a specif...  \n",
       "2072  Reflex response\\nCauses reciprocal inhibition ...  \n",
       "2073  It’s an exaggerated reflex Often caused by a m...  \n",
       "2074                                     Benzodiazepine  \n",
       "2075                                      Tetanus toxin  \n",
       "\n",
       "[2076 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df = pd.read_csv('qa.csv')\n",
    "qa_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 1286.99it/s]\n"
     ]
    }
   ],
   "source": [
    "docs_dir = Path('./docs')\n",
    "loaders = []\n",
    "\n",
    "for file in tqdm(docs_dir.iterdir()):\n",
    "  if file.is_file() and file.suffix == '.pdf':\n",
    "    loader = PyPDFLoader(file)\n",
    "    loaders.append(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama3():\n",
    "  return Ollama(temperature=0, model='llama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openbiollm_parser(output):\n",
    "  query = output['query']\n",
    "  result = output['result']\n",
    "  idx = result.find('Helpful Answer: ')\n",
    "  if idx != -1:\n",
    "    return {'query': query, 'result': result[idx + len('Helpful answer: '):]}\n",
    "  else:\n",
    "    return {'query': query, 'result': result}\n",
    "\n",
    "def get_openbiollm_8b():\n",
    "  model = 'aaditya/OpenBioLLM-Llama3-8B'\n",
    "  model_kwargs = {'torch_dtype': torch.bfloat16}\n",
    "  pipeline = transformers.pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    device=device,\n",
    "  )\n",
    "  terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids('<|eot_id|>')\n",
    "  ]\n",
    "  llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model,\n",
    "    task='text-generation',\n",
    "    model_kwargs=model_kwargs,\n",
    "    pipeline_kwargs={\n",
    "      'max_new_tokens': 256,\n",
    "      'eos_token_id': terminators,\n",
    "      'do_sample': True,\n",
    "      'temperature': 0.01,\n",
    "      'top_p': 0.9,\n",
    "    },\n",
    "  )\n",
    "  return llm | openbiollm_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup index stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_array_in_memory_search(loaders=[]):\n",
    "  index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embeddings,\n",
    "  ).from_loaders(loaders)\n",
    "  return index.vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = [\n",
    "  ('LLaMA 3', get_llama3()),\n",
    "  # ('OpenBioLLM Llama3 8B', get_openbiollm_8b()),\n",
    "]\n",
    "\n",
    "vector_stores = [\n",
    "  ('Doc Array In Memory Search', get_doc_array_in_memory_search),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = Path('cache.json')\n",
    "with open(cache_path, 'r') as f:\n",
    "  cache = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLMs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/zmq/backend/cython/checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 13, in zmq.backend.cython.checkrc._check_rc\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['llm', 'vector_store', 'use_docs', 'accuracy'])\n",
    "\n",
    "sample_df = qa_df.sample(frac=0.1)\n",
    "questions = sample_df['question'].tolist()\n",
    "expected_answers = sample_df['answer'].tolist()\n",
    "\n",
    "for llm_name, llm in tqdm(llms, desc='LLMs'):\n",
    "  for vector_store_name, get_vector_store in tqdm(vector_stores, desc='Vector Stores', leave=False):\n",
    "    for use_docs in tqdm((True, False), desc='Use Docs', leave=False):\n",
    "      vector_store = get_vector_store(loaders if use_docs else [])\n",
    "      qa_llm = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type='stuff',\n",
    "        retriever=vector_store.as_retriever(),\n",
    "        verbose=True,\n",
    "        chain_type_kwargs = {'document_separator': '<<<<>>>>>'},\n",
    "      )\n",
    "\n",
    "      predicted_answers = []\n",
    "\n",
    "      for index, question in tqdm(enumerate(questions), desc='Questions', leave=False):\n",
    "        key = f'{llm_name}_{vector_store_name}_{use_docs}'\n",
    "\n",
    "        if not key in cache[question]:\n",
    "          cache[key] = {}\n",
    "\n",
    "        if not question in cache[key]:\n",
    "          cache[key][question] = qa_llm.invoke(question)['result']\n",
    "\n",
    "        predicted_answers.append(cache[key])\n",
    "\n",
    "        with open(cache_path, 'w') as f:\n",
    "          json.dump(cache, f)\n",
    "\n",
    "      accuracy = embeddings_cosine_sim_metric(expected_answers, predicted_answers)\n",
    "\n",
    "      row = pd.DataFrame({\n",
    "        'llm': llm_name,\n",
    "        'vector_store': vector_store_name,\n",
    "        'use_docs': use_docs,\n",
    "        'accuracy': accuracy,\n",
    "      })\n",
    "      df = pd.concat([df, row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
